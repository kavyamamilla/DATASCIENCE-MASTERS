{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Gradient Boosting Regression is a machine learning technique that belongs to the ensemble learning family. It's a popular and powerful method used for regression tasks, where the goal is to predict continuous numerical values based on input features. Gradient Boosting Regression builds a strong predictive model by combining the predictions of multiple weak learners (usually decision trees) in an additive manner.\n",
        "\n",
        "Here's how Gradient Boosting Regression works:\n",
        "\n",
        "Weak Learners: Gradient Boosting starts with a single weak learner, typically a shallow decision tree with limited depth. This tree makes initial predictions, which are often far from accurate.\n",
        "\n",
        "Residual Calculation: The differences (residuals) between the actual target values and the initial predictions are calculated for each data point in the training set.\n",
        "\n",
        "Fit New Learner: A new weak learner (another decision tree) is trained to predict these residuals. This new learner tries to capture the patterns in the errors made by the previous model.\n",
        "\n",
        "Update Predictions: The predictions from the new weak learner are added to the previous predictions, adjusting the model's predictions to reduce the residual errors.\n",
        "\n",
        "Iteration: Steps 3 and 4 are repeated iteratively, with each new learner trying to capture and correct the remaining errors made by the previous models. The predictions are updated in an additive manner.\n",
        "\n",
        "Final Prediction: The final prediction is the sum of the predictions from all the weak learners. This cumulative effect of multiple models creates a strong predictive model that captures complex relationships in the data.\n",
        "\n",
        "The \"gradient\" in Gradient Boosting refers to the fact that at each iteration, the new weak learner is trained to fit the negative gradient of the loss function with respect to the current predictions. This approach guides the new learner's focus on the data points that the previous model struggled to predict accurately.\n",
        "\n",
        "Gradient Boosting Regression offers several advantages:\n",
        "\n",
        "High Predictive Power: Gradient Boosting can capture complex relationships in the data, making it suitable for a wide range of regression tasks.\n",
        "\n",
        "Handles Nonlinearities: It can model nonlinear relationships without requiring complex feature engineering.\n",
        "\n",
        "Handles Noisy Data: Gradient Boosting can handle noisy data and outliers well.\n",
        "\n",
        "Ensemble of Weak Learners: By combining multiple weak learners, it reduces overfitting and enhances generalization.\n",
        "\n",
        "However, Gradient Boosting Regression also has some considerations:\n",
        "\n",
        "Computationally Intensive: It can be computationally intensive, especially when the number of iterations or depth of trees is large.\n",
        "\n",
        "Hyperparameter Tuning: It requires careful hyperparameter tuning to prevent overfitting and ensure optimal performance.\n",
        "\n",
        "Interpretability: The final model may be complex, making it less interpretable compared to simpler models.\n",
        "\n",
        "Gradient Boosting Regression is implemented in various machine learning libraries, such as scikit-learn (Python), XGBoost, LightGBM, and CatBoost, each offering optimizations and enhancements to the basic algorithm."
      ],
      "metadata": {
        "id": "T3UWLPjG9n5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Generate a simple dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(50, 1) * 10\n",
        "y = 2 * X + 1 + np.random.randn(50, 1)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the Gradient Boosting Regression algorithm\n",
        "class GradientBoostingRegressor:\n",
        "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
        "        self.n_estimators = n_estimators\n",
        "        self.learning_rate = learning_rate\n",
        "        self.models = []\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        y_pred = np.full(y.shape, np.mean(y))  # Initialize predictions with mean\n",
        "        for _ in range(self.n_estimators):\n",
        "            residuals = y - y_pred\n",
        "            model = DecisionTreeRegressor(max_depth=1)  # Weak learner (shallow decision tree)\n",
        "            model.fit(X, residuals)\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "            self.models.append(model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        y_pred = np.zeros(X.shape[0])\n",
        "        for model in self.models:\n",
        "            y_pred += self.learning_rate * model.predict(X)\n",
        "        return y_pred\n",
        "\n",
        "# Define a simple decision tree regressor\n",
        "class DecisionTreeRegressor:\n",
        "    def __init__(self, max_depth=None):\n",
        "        self.max_depth = max_depth\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.feature_index, self.threshold, self.left, self.right = self._split(X, y, depth=0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return [self._predict(x) for x in X]\n",
        "\n",
        "    def _split(self, X, y, depth):\n",
        "        if depth >= self.max_depth:\n",
        "            return None, np.mean(y), None, None\n",
        "\n",
        "        num_samples, num_features = X.shape\n",
        "        if num_samples <= 1:\n",
        "            return None, np.mean(y), None, None\n",
        "\n",
        "        variance = np.var(y)\n",
        "        best_variance_reduction = 0\n",
        "        best_feature_index = None\n",
        "        best_threshold = None\n",
        "        for feature_index in range(num_features):\n",
        "            thresholds = np.unique(X[:, feature_index])\n",
        "            for threshold in thresholds:\n",
        "                y_left = y[X[:, feature_index] <= threshold]\n",
        "                y_right = y[X[:, feature_index] > threshold]\n",
        "                if len(y_left) == 0 or len(y_right) == 0:\n",
        "                    continue\n",
        "                current_variance_reduction = variance - (len(y_left) / num_samples) * np.var(y_left) - \\\n",
        "                    (len(y_right) / num_samples) * np.var(y_right)\n",
        "                if current_variance_reduction > best_variance_reduction:\n",
        "                    best_variance_reduction = current_variance_reduction\n",
        "                    best_feature_index = feature_index\n",
        "                    best_threshold = threshold\n",
        "\n",
        "        if best_variance_reduction == 0:\n",
        "            return None, np.mean(y), None, None\n",
        "\n",
        "        X_left = X[X[:, best_feature_index] <= best_threshold]\n",
        "        y_left = y[X[:, best_feature_index] <= best_threshold]\n",
        "        X_right = X[X[:, best_feature_index] > best_threshold]\n",
        "        y_right = y[X[:, best_feature_index] > best_threshold]\n",
        "        left = self._split(X_left, y_left, depth + 1)\n",
        "        right = self._split(X_right, y_right, depth + 1)\n",
        "        return best_feature_index, best_threshold, left, right\n",
        "\n",
        "    def _predict(self, x, tree=None):\n",
        "        if tree is None:\n",
        "            tree = self\n",
        "        if tree.left is None and tree.right is None:\n",
        "            return tree.threshold\n",
        "        if x[tree.feature_index] <= tree.threshold:\n",
        "            return self._predict(x, tree.left)\n",
        "        else:\n",
        "            return self._predict(x, tree.right)\n",
        "\n",
        "# Train the Gradient Boosting model\n",
        "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
        "gb_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gb_regressor.predict(X_test)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "OQZN5Gcm9pF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we implemented a simple Gradient Boosting Regression algorithm from scratch using NumPy. We also defined a basic Decision Tree Regressor to be used as weak learners within the gradient boosting process. The model is trained on a small dataset,"
      ],
      "metadata": {
        "id": "-W6h3EQ29r5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# Generate a simple dataset\n",
        "np.random.seed(42)\n",
        "X = np.random.rand(50, 1) * 10\n",
        "y = 2 * X + 1 + np.random.randn(50, 1)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the parameter grid for grid search\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [1, 2, 3]\n",
        "}\n",
        "\n",
        "# Initialize the Gradient Boosting Regressor\n",
        "gb_regressor = GradientBoostingRegressor()\n",
        "\n",
        "# Perform grid search using cross-validation\n",
        "grid_search = GridSearchCV(gb_regressor, param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train.ravel())\n",
        "\n",
        "# Get the best hyperparameters from grid search\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(f\"R-squared: {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "QGXqrRy-904A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this code, we've replaced our custom Gradient Boosting implementation with scikit-learn's GradientBoostingRegressor. We use GridSearchCV to perform a grid search over the specified parameter grid (param_grid). The best hyperparameters are printed, and the best model is used to make predictions and evaluate its performance on the test set."
      ],
      "metadata": {
        "id": "y7VrCB1O916q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "4. In Gradient Boosting, a \"weak learner\" refers to a relatively simple and low-complexity model that performs slightly better than random guessing on a given task. Weak learners are often used as the base or building block models in the ensemble learning process of Gradient Boosting.\n",
        "\n",
        "The concept of using weak learners in Gradient Boosting is essential for its success. The idea is that by sequentially combining multiple weak learners, each focusing on correcting the errors made by the previous models, the ensemble model (final Gradient Boosting model) becomes a strong learner that can capture complex relationships within the data.\n",
        "\n",
        "In the context of Gradient Boosting:\n",
        "\n",
        "Weak Learner's Simplicity: Weak learners are typically simple models with low complexity. Examples include shallow decision trees (often referred to as \"stumps\"), linear regression models, or models with a small number of features.\n",
        "\n",
        "Sequential Learning: Gradient Boosting trains weak learners sequentially. Each new weak learner focuses on minimizing the errors made by the ensemble model formed by the previous learners.\n",
        "\n",
        "Emphasis on Errors: The new weak learner is designed to capture the patterns in the errors or residuals made by the current ensemble of models. It focuses on the data points where the ensemble model is making significant errors.\n",
        "\n",
        "Additive Nature: The predictions from weak learners are combined additively. Each new weak learner contributes to refining the predictions made by the previous models, gradually improving the overall model's performance.\n",
        "\n",
        "By using weak learners and iteratively improving upon their predictions, Gradient Boosting effectively constructs a strong model that can adapt to complex data relationships. It's important to note that even though individual weak learners might not perform exceptionally well, their collective power, when combined properly, leads to a highly accurate ensemble model."
      ],
      "metadata": {
        "id": "5FOmvl4795ri"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. The intuition behind the Gradient Boosting algorithm lies in its ability to combine the predictive power of multiple weak learners (simple models) to create a strong ensemble model that can accurately capture complex relationships within the data. Here's a step-by-step intuition of how Gradient Boosting works:\n",
        "\n",
        "Initialization: The algorithm starts with an initial prediction for the target variable. This initial prediction is usually set to a simple value, such as the mean of the target variable.\n",
        "\n",
        "Sequential Learning: The algorithm iteratively improves the initial prediction by adding new weak learners one at a time. Each new learner is focused on correcting the errors made by the ensemble of previous learners.\n",
        "\n",
        "Error Emphasis: At each iteration, the algorithm calculates the difference between the true target values and the current ensemble's predictions. These differences (residuals) indicate where the current model is making errors.\n",
        "\n",
        "Weak Learner Training: A new weak learner is trained to predict these residuals. This learner is designed to capture the patterns in the errors made by the current ensemble.\n",
        "\n",
        "Updating Predictions: The new weak learner's predictions are added to the ensemble's current predictions. These new predictions gradually improve the ensemble's accuracy by reducing the errors made by the previous models.\n",
        "\n",
        "Learning Rate: The algorithm introduces a learning rate that controls the contribution of each new weak learner's predictions. A smaller learning rate can prevent the ensemble from overfitting and provide more stable convergence.\n",
        "\n",
        "Iteration: Steps 3 to 6 are repeated for a predefined number of iterations (controlled by hyperparameters like the number of trees). In each iteration, a new weak learner is added, and its predictions are combined with the previous ensemble's predictions.\n",
        "\n",
        "Final Prediction: The final ensemble model is the sum of the predictions from all weak learners, adjusted by the learning rate. This ensemble model captures the complex relationships in the data by iteratively refining its predictions.\n",
        "\n",
        "The intuition behind Gradient Boosting is that it focuses on the data points where the current ensemble model is making the most errors. By training new weak learners to correct these errors, the algorithm gradually builds a powerful ensemble model that can adapt to intricate patterns and relationships in the data.\n",
        "\n",
        "Overall, Gradient Boosting's key strengths lie in its ability to handle complex data, its adaptability to various types of tasks (classification and regression), and its effectiveness in producing accurate predictions by combining the contributions of multiple simple models."
      ],
      "metadata": {
        "id": "_EZ5XNfP-Iv3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. The Gradient Boosting algorithm builds an ensemble of weak learners through an iterative process. Each weak learner is trained to correct the errors made by the ensemble of previously trained learners. The ensemble is constructed by sequentially adding new weak learners, and each learner contributes to refining the predictions made by the previous learners. Here's how the process works:\n",
        "\n",
        "Initialization: The ensemble starts with an initial prediction, which is often set to a simple value like the mean of the target variable.\n",
        "\n",
        "Compute Residuals: The algorithm calculates the differences between the true target values and the current ensemble's predictions. These differences are the residuals, indicating the errors made by the current model.\n",
        "\n",
        "Train a Weak Learner: A new weak learner (e.g., a decision tree) is trained to predict the residuals. This weak learner is designed to capture the patterns in the errors made by the current ensemble.\n",
        "\n",
        "Predict Residuals: The new weak learner's predictions for the residuals are added to the current ensemble's predictions.\n",
        "\n",
        "Update Ensemble Predictions: The current ensemble's predictions are updated by adding the predictions of the new weak learner. The result is a new set of predictions that have been adjusted to reduce the errors made by the previous ensemble.\n",
        "\n",
        "Learning Rate: A learning rate parameter controls the contribution of each new weak learner's predictions. A smaller learning rate prevents the model from overfitting and provides more stable convergence.\n",
        "\n",
        "Iteration: Steps 2 to 6 are repeated for a predefined number of iterations (controlled by hyperparameters like the number of trees). In each iteration, a new weak learner is trained, and its predictions are combined with the previous ensemble's predictions.\n",
        "\n",
        "Final Prediction: The final ensemble model is the sum of the predictions from all weak learners, adjusted by the learning rate. This ensemble captures the complex relationships in the data by iteratively refining its predictions.\n",
        "\n",
        "The ensemble is constructed by gradually adding new learners that focus on the data points where the current ensemble is making errors. Each new learner's predictions are integrated into the ensemble, improving the model's accuracy by addressing the deficiencies of the previous ensemble. This iterative process of training and updating weak learners allows Gradient Boosting to create a strong ensemble model that can capture complex relationships within the data."
      ],
      "metadata": {
        "id": "lveJYZQb-CzF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding how the algorithm iteratively combines weak learners to form a strong ensemble model. Let's break down the mathematical steps involved in the algorithm's intuition:\n",
        "\n",
        "Initialization:\n",
        "\n",
        "Initialize the ensemble's predictions, often with a simple value like the mean of the target variable:\n",
        "F0(x)=initial value.\n",
        "Compute Residuals:\n",
        "\n",
        "Calculate the residuals (errors) between the true target values y and the current ensemble's predictions:\n",
        "r i=y i−F prev(x i), where F prev(x i) is the previous ensemble's prediction for the ith data point.\n",
        "Train a Weak Learner:\n",
        "\n",
        "Train a new weak learner, such as a decision tree, to predict the residuals r i.This new learner captures the patterns in the errors made by the current ensemble:\n",
        "h i(x)=weak learner(x;r i).\n",
        "Predict Residuals:\n",
        "\n",
        "Use the new weak learner to predict the residuals for all data points:\n",
        "ri^=h i(x i).\n",
        "Update Ensemble Predictions:\n",
        "Update the ensemble's predictions by adding the predictions of the new weak learner to the previous ensemble's predictions:\n",
        "F new(x)=F prev(x)+λ⋅h i(x), where λ is the learning rate.\n",
        "Iteration:\n",
        "\n",
        "Repeat steps 2 to 5 for a predefined number of iterations or until convergence.\n",
        "Final Prediction:\n",
        "\n",
        "The final ensemble model is the sum of the predictions from all the weak learners, each scaled by the learning rate:\n",
        "F final(x)=F 0(x)+∑ i=1 to T λ⋅h i(x), where T is the total number of iterations.\n",
        "Mathematically, the Gradient Boosting algorithm aims to find a sequence of weak learners h i(x) such that the ensemble F final(x) approximates the true target values y as closely as possible. Each weak learner contributes to the ensemble by addressing the errors and patterns missed by the previous learners. The learning rate λ controls the contribution of each weak learner's predictions and helps prevent overfitting.\n",
        "\n",
        "The algorithm's intuition is rooted in minimizing the residuals at each step, iteratively improving the ensemble's predictions by incorporating the insights gained from the new weak learners. This process allows Gradient Boosting to create a powerful predictive model that adapts to complex data relationships.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UmEaiYBe-S-0"
      }
    }
  ]
}