{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d88522b5-4f90-4aac-9e40-0e176220b6c0",
   "metadata": {},
   "source": [
    "### Web scraping refers to the extraction of data from a website. This information is collected and then exported into a format that is more useful for the user.The uses of Web Scraping for business as well as personal requirements are endless. Each business or individual has their own specific need for gathering data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef1897a-c48d-48bb-8709-f2ae8ece5a67",
   "metadata": {},
   "source": [
    "### Price Comparison & Competition Monitoring.E-Commerce. Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abf7591-1317-447c-8074-631d08d82cb5",
   "metadata": {},
   "source": [
    "### \n",
    "Human Copy-and-Paste\n",
    "\n",
    "Manually copying and pasting data from a web page into a text file or spreadsheet is the most basic form of web scraping. Even the best web-scraping technology cannot always replace a human’s manual examination and copy-and-paste, and this may be the only viable option when the websites for scraping explicitly prohibit machine automation.\n",
    "\n",
    "Text Pattern Matching\n",
    "\n",
    "The UNIX grep command or regular expression-matching facilities of programming languages can be used to extract information from web pages in a simple yet powerful way (for instance Perl or Python).\n",
    "HTTP Programming\n",
    "\n",
    "Static and dynamic web pages can be retrieved by using socket programming to send HTTP requests to a remote web server.\n",
    "\n",
    "HTML Parsing\n",
    "\n",
    "Many websites contain large collections of pages that are dynamically generated from an underlying structured source, such as a database. A common script or template is typically used to encode data from the same category into similar pages. A wrapper is a program in data mining that detects such templates in a specific information source, extracts its content, and converts it to a relational form.\n",
    "\n",
    "Wrapper generation algorithms assume that the input pages of a wrapper induction system follow a common template and can be identified using a URL common scheme. [2] Furthermore, semi-structured data query languages such as XQuery and HTQL can be used to parse HTML pages as well as retrieve and transform page content.\n",
    "\n",
    "DOM Parsing\n",
    "\n",
    "More information: Object Model for Documents, Programs can retrieve dynamic content generated by client-side scripts by embedding a full-fledged web browser, such as Internet Explorer or the Mozilla browser control. These browser controls also parse web pages into a DOM tree, which programs can use to retrieve portions of the pages. The resulting DOM tree can be parsed using languages such as Xpath.\n",
    "\n",
    "Vertical Aggregation\n",
    "\n",
    "Several companies have created vertically specific harvesting platforms. These platforms generate and monitor a plethora of “bots” for specific verticals with no “man in the loop” (direct human involvement) and no work related to a specific target site. The preparation entails creating a knowledge base for the entire vertical, after which the platform will create the bots automatically.\n",
    "\n",
    "The robustness of the platform is measured by the quality of the information it retrieves (typically the number of fields) and its scalability (how quickly it can scale up to hundreds or thousands of sites). This scalability is primarily used to target the Long Tail of sites that common aggregators find too difficult or time-consuming to harvest content from.\n",
    "\n",
    "Semantic Annotation Recognizing\n",
    "\n",
    "The scraped pages may include metadata, semantic markups, and annotations that can be used to locate specific data snippets. This technique can be viewed as a subset of DOM parsing if the annotations are embedded in the pages, as Microformat does. In another case, the annotations are stored and managed separately from the web pages, so scrapers can retrieve data schema and instructions from this layer before scraping the pages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b5ead5-fbea-44c7-837e-66503be1510f",
   "metadata": {},
   "source": [
    "### Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cd806d-88c0-478a-bfc6-b14399122c7f",
   "metadata": {},
   "source": [
    "### Uses of Beautiful Soup\n",
    "The Beautiful Soup library helps with isolating titles and links from webpages. It can extract all of the text from ​HTML tags, and alter the HTML ​in the document with which we’re working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd36a4-fbea-46df-ba2d-8efb1cc73a1f",
   "metadata": {},
   "source": [
    "### Features of Beautiful Soup\n",
    "Some key features that make beautiful soup unique are:\n",
    "\n",
    "Beautiful Soup provides a few simple methods and Pythonic idioms for navigating, searching, and modifying a parse tree.\n",
    "\n",
    "Beautiful Soup automatically converts incoming documents to Unicode and outgoing documents to UTF-8.\n",
    "\n",
    "Beautiful Soup sits on top of popular Python parsers like lxml and html5lib, which allows​ us to try out different parsing strategies or trade speed for flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21ce19-26ad-4ff9-ab74-a26ed54fe2f2",
   "metadata": {},
   "source": [
    "### Web scraping is the process of using bots to extract content and data from a website. Unlike screen scraping, which only copies pixels displayed onscreen, web scraping extracts underlying HTML code and, with it, data stored in a database. The scraper can then replicate entire website content elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b907f577-e485-44e3-8847-772b99e96319",
   "metadata": {},
   "source": [
    "### Flask is a lightweight framework to build websites. We'll use this to parse our collected data and display it as HTML in a new HTML file. The requests module allows us to send http requests to the website we want to scrape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b273b933-122a-47a3-b90b-ec071eefc793",
   "metadata": {},
   "source": [
    "### Developers have to deal with enough headaches as it is. AWS Beanstalk was created to help developers manage website infrastructure. It’s difficult for developers to switch from development to maintenance at the drop of a hat. Yet, AWS Beanstalk offers autoscaling to ensure automatic updates of new software. And, this service runs automatically.AWS Beanstalk really is a timesaver. It automates the setup, configuration, and provisioning of other AWS services such as EC2, RDS, and S3. Not to mention, the automated setup also helps to mitigate human error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01cacdc-b680-4918-97e5-9a703ad49d7b",
   "metadata": {},
   "source": [
    "### AWS CodePipeline is a fully managed continuous delivery service that helps users automate release pipelines for fast, reliable application and infrastructure updates. CodePipeline automates the build, test, and deploy phases of the release process every time there is a code change, based on the release model a user defines. This is to enable rapid, reliable delivery of features and updates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3e6306-794f-4c9d-8146-f887ec6d423d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "import logging"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
