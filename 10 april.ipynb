{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. To calculate the probability that an employee is a smoker given that they use the health insurance plan, we can use the conditional probability formula:\n",
        "P(Smoker | Uses Health Insurance Plan)= P(Smoker and Uses Health Insurance Plan)/P(Uses Health Insurance Plan)\n",
        "P(Uses Health Insurance Plan)=0.70\n",
        "P(Smoker)=0.40\n",
        "We need to find P(Smoker and Uses Health Insurance Plan), which is the probability that an employee is both a smoker and uses the health insurance plan. Since these events are not independent, we'll use the fact that:\n",
        "P(Smoker and Uses Health Insurance Plan)=P(Smoker)×P(Uses Health Insurance Plan | Smoker)\n",
        "\n",
        "Given that 40% of the employees who use the plan are smokers, we have\n",
        "P(Uses Health Insurance Plan | Smoker)=0.40.\n",
        "\n",
        "Now we can substitute these values into the conditional probability formula to calculate the probability that an employee is a smoker given that they use the health insurance plan:\n",
        "P(Smoker | Uses Health Insurance Plan)= P(Smoker and Uses Health Insurance Plan)/P(Uses Health Insurance Plan)=P(Smoker)×P(Uses Health Insurance Plan | Smoker)/P(Uses Health Insurance Plan)\n",
        "P(Smoker | Uses Health Insurance Plan)= 0.40×0.40/0.70\n",
        "P(Smoker | Uses Health Insurance Plan)= 0.16/0.70≈0.2286\n",
        "\n",
        "So, the probability that an employee is a smoker given that they use the health insurance plan is approximately 0.2286 or 22.86%."
      ],
      "metadata": {
        "id": "b7ClH8X3pMvk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.\n",
        "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes algorithm, a popular probabilistic classification technique. They are used in different contexts and assumptions about the data. Here's the difference between them:\n",
        "\n",
        "1. Bernoulli Naive Bayes:\n",
        "\n",
        "Bernoulli Naive Bayes is designed for binary or boolean features, where each feature can take on one of two values (usually 0 or 1). It's commonly used when dealing with text data, where the presence or absence of a word is represented by a binary value.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Assumes binary features.\n",
        "Useful for text classification where each feature represents the presence or absence of a word in a document.\n",
        "Ignores the frequency or count of features and only considers their presence or absence.\n",
        "Often used for tasks like sentiment analysis, spam detection, and document categorization.\n",
        "2. Multinomial Naive Bayes:\n",
        "\n",
        "Multinomial Naive Bayes is used when dealing with discrete data, especially when features represent counts or frequencies. It's widely used for text classification where features can represent the frequency of words in a document.\n",
        "\n",
        "Key Characteristics:\n",
        "\n",
        "Suited for discrete data, such as counts or frequencies.\n",
        "Commonly used in text classification tasks, where features can represent the frequency of words in a document.\n",
        "Considers the frequency or count of features while calculating probabilities.\n",
        "Can handle multiple classes or categories.\n",
        "Similarities:\n",
        "Both Bernoulli Naive Bayes and Multinomial Naive Bayes are based on the same underlying Naive Bayes algorithm, which applies Bayes' theorem and the \"naive\" assumption that features are conditionally independent given the class label. Both algorithms are suitable for text classification problems and are simple to implement and computationally efficient.\n",
        "\n",
        "Choosing Between Them:\n",
        "The choice between Bernoulli and Multinomial Naive Bayes depends on the nature of your data. If your features are binary (presence or absence) and you're dealing with text classification, Bernoulli Naive Bayes might be more appropriate. If your features represent counts or frequencies and you're working with text data, Multinomial Naive Bayes might be a better fit."
      ],
      "metadata": {
        "id": "wYmkB-8ktz70"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Bernoulli Naive Bayes, like other variants of the Naive Bayes algorithm, requires complete data with no missing values. This is because the algorithm relies on the presence or absence of features to calculate probabilities. In Bernoulli Naive Bayes, features are typically binary, representing the presence (1) or absence (0) of a certain attribute.\n",
        "\n",
        "When dealing with missing values in Bernoulli Naive Bayes, you generally have a few options:\n",
        "\n",
        "Imputation: Impute missing values with a default value that doesn't disrupt the binary nature of the features. For example, you might choose to impute missing values with 0 to indicate the absence of the feature. However, this approach should be used cautiously, as it could introduce bias or alter the distribution of the data.\n",
        "\n",
        "Feature Engineering: If the reason for missing values is related to a certain pattern or property of the data, you could consider creating a new binary feature to represent the presence or absence of the missing attribute. This might help the algorithm capture potential information from the missingness itself.\n",
        "\n",
        "Data Transformation: Depending on the nature of your data and the missingness, you could consider transforming your dataset into a format that doesn't rely on binary features. For example, you might use Multinomial Naive Bayes, which can handle discrete features with multiple levels, and then apply imputation or other techniques for handling missing values.\n",
        "\n",
        "Exclusion: If the proportion of missing values is relatively small and doesn't significantly impact the overall dataset, you might choose to exclude instances with missing values from your analysis. However, this should be done carefully, considering potential biases introduced by excluding data."
      ],
      "metadata": {
        "id": "QHIYd5E8t7Id"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Yes, Gaussian Naive Bayes can be used for multi-class classification. Gaussian Naive Bayes is an extension of the Naive Bayes algorithm that assumes continuous features and follows a Gaussian (normal) distribution. While it's often used for binary classification, it can also be adapted for multi-class classification by extending the algorithm to handle multiple classes.\n",
        "\n",
        "Here's how Gaussian Naive Bayes can be used for multi-class classification:\n",
        "\n",
        "Data Preparation: Prepare your dataset with continuous features. Each feature is assumed to follow a Gaussian distribution within each class.\n",
        "\n",
        "Parameter Estimation: For each class, estimate the mean and variance of each feature based on the data samples belonging to that class. This involves calculating the mean and variance for each feature separately within each class.\n",
        "\n",
        "Class Prior Probability: Calculate the prior probability of each class based on the proportion of instances in each class in the training data.\n",
        "\n",
        "Prediction: Given a new instance with continuous features, calculate the likelihood of the features within each class using the Gaussian distribution parameters (mean and variance) estimated in step 2. Then, combine the likelihood with the prior probability of each class to compute the posterior probability using Bayes' theorem. The class with the highest posterior probability is predicted as the output class.\n",
        "\n",
        "Handling Independence Assumption: While Gaussian Naive Bayes assumes features are independent within each class, this assumption might not hold in some cases. Despite this limitation, Gaussian Naive Bayes can still perform reasonably well, especially when the correlations between features are not strong.\n",
        "\n",
        "Gaussian Naive Bayes can handle more than two classes by extending the algorithm to consider all classes simultaneously during the prediction step. The class with the highest posterior probability is chosen as the predicted class.\n",
        "\n",
        "It's important to note that the assumption of features being Gaussian-distributed might not hold for all datasets. Careful validation and experimentation are necessary to determine whether Gaussian Naive Bayes is suitable for your specific multi-class classification problem. If the assumption doesn't hold, you might consider other variants of Naive Bayes, such as Multinomial or Complement Naive Bayes, which are more appropriate for discrete or non-Gaussian data."
      ],
      "metadata": {
        "id": "c3X-jZ_kuC_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"spambase.data\", header=None)\n",
        "X = data.iloc[:, :-1].values\n",
        "y = data.iloc[:, -1].values\n",
        "\n",
        "# Initialize classifiers\n",
        "bernoulli_classifier = BernoulliNB()\n",
        "multinomial_classifier = MultinomialNB()\n",
        "gaussian_classifier = GaussianNB()\n",
        "\n",
        "# Cross-validation\n",
        "def evaluate_classifier(classifier):\n",
        "    accuracy = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='accuracy'))\n",
        "    precision = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='precision'))\n",
        "    recall = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='recall'))\n",
        "    f1 = np.mean(cross_val_score(classifier, X, y, cv=10, scoring='f1'))\n",
        "    return accuracy, precision, recall, f1\n",
        "\n",
        "# Evaluate each classifier\n",
        "bernoulli_accuracy, bernoulli_precision, bernoulli_recall, bernoulli_f1 = evaluate_classifier(bernoulli_classifier)\n",
        "multinomial_accuracy, multinomial_precision, multinomial_recall, multinomial_f1 = evaluate_classifier(multinomial_classifier)\n",
        "gaussian_accuracy, gaussian_precision, gaussian_recall, gaussian_f1 = evaluate_classifier(gaussian_classifier)\n",
        "\n",
        "# Print results\n",
        "print(\"Bernoulli Naive Bayes:\")\n",
        "print(\"Accuracy:\", bernoulli_accuracy)\n",
        "print(\"Precision:\", bernoulli_precision)\n",
        "print(\"Recall:\", bernoulli_recall)\n",
        "print(\"F1 Score:\", bernoulli_f1)\n",
        "print()\n",
        "\n",
        "print(\"Multinomial Naive Bayes:\")\n",
        "print(\"Accuracy:\", multinomial_accuracy)\n",
        "print(\"Precision:\", multinomial_precision)\n",
        "print(\"Recall:\", multinomial_recall)\n",
        "print(\"F1 Score:\", multinomial_f1)\n",
        "print()\n",
        "\n",
        "print(\"Gaussian Naive Bayes:\")\n",
        "print(\"Accuracy:\", gaussian_accuracy)\n",
        "print(\"Precision:\", gaussian_precision)\n",
        "print(\"Recall:\", gaussian_recall)\n",
        "print(\"F1 Score:\", gaussian_f1)\n"
      ],
      "metadata": {
        "id": "XGJgYnfkudUl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}